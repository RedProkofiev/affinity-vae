{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314eccba",
   "metadata": {},
   "source": [
    "# Post processing notebook \n",
    "\n",
    "### This notebook import the visualisation functions from the main library and loads the state and meta file and allows you to modify the appearance of the output of Affinity-VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be55a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, fixed, Dropdown\n",
    "from avae.vis import latent_embed_plot_umap, latent_embed_plot_tsne\n",
    "from avae.utils import colour_per_class\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf4e2d",
   "metadata": {},
   "source": [
    "### The following functions take in the path of the meta and the state file respectively and load them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(meta_fn, device=\"cpu\"):\n",
    "    meta_df = pd.read_pickle(meta_fn)\n",
    "\n",
    "    mu = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"lat\")]\n",
    "    ].to_numpy()  # Assuming the column name for latent variables is 'latent'\n",
    "    \n",
    "    labels = meta_df[\"id\"]\n",
    "\n",
    "    pose = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"pos\")]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    std = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"std\")]\n",
    "    ].to_numpy()\n",
    "    std = std[:,1:]\n",
    "\n",
    "    z =  np.random.randn(*std.shape)  * std + mu\n",
    "    return mu, pose, labels, std, z\n",
    "    \n",
    "\n",
    "def load_model(model_fn, device=\"cpu\"):\n",
    "    checkpoint = torch.load(model_fn, map_location=torch.device(device))\n",
    "    model = checkpoint[\"model_class_object\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed324a89",
   "metadata": {},
   "source": [
    "# Enter the path for the saved model and corresponding meta file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74c5c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = 'path/to/model'\n",
    "meta_fn = 'path/to/meta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcb862",
   "metadata": {},
   "source": [
    "# Grab the Information in the meta file and load the model\n",
    "### From the meta file \n",
    "1. `mu`: mean of the latents ($\\mu$)\n",
    "2. `std`: standard deviation of the latents ($\\sigma$)\n",
    "3. `p`: $pose$\n",
    "4. `labels`: class labels\n",
    "5. `z`: sampled latent ($z$) \n",
    "### model loaded to `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26168d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, p, labels, std, z = load_meta(meta_fn)\n",
    "model =load_model(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cebd8",
   "metadata": {},
   "source": [
    "# Configure latent embedding:\n",
    "#### select the following to plot the UMAP embedding of the latent space : \n",
    "1. `select_function`: The drop down list allows you to choose between UMAP and TSNE for your dimentionality reduction of the latent embeddings. \n",
    "2. `rs` : This slider sets the random state for the UMAP plot.\n",
    "3. `perplexity`: This slider sets the perplexity for the TSNE plot.\n",
    "4. `Data Type`: This drop down list allows you to select which variable to plot (`z` : stochastic sample of latent space, `mu`: the mean of the latent space)\n",
    "5. `class_list_order`: This variable ensures that the colours used in the plot are picked in the order you ahve provided (for consistency when produing more than one plot), you can copy the list from the output of affinity-vae or leave as `None` if you dont need this variable.  \n",
    "6. `marker_size`: adjust the size of markers for each data point to have the desired plot appearence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "472410a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b4e84038584b418ce86a7214b426e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select Function', options=('latent_embed_plot_umap', 'latent_embed…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(function, data_type, rs, perplexity)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the slider widget\n",
    "random_state_slider = IntSlider(min=0, max=100, step=1, value=42)\n",
    "perplexity_slider = IntSlider(min=2, max=100, step=1, value=40, description='Perplexity')\n",
    "class_list_order = ['1S3X', '3QM1', '3GL1', '3H84', '2CG9', '3D2F', '1U6G', '3CF3', '1BXN', '1QVR', '4CR2', '5MRC', 'fidu']\n",
    "# Define the dropdown widget for selecting the function\n",
    "function_selector = Dropdown(options=['latent_embed_plot_umap', 'latent_embed_plot_tsne'],\n",
    "                             value='latent_embed_plot_umap', description='Select Function')\n",
    "\n",
    "# Use the interact function with both widgets\n",
    "\n",
    "interact(lambda function, data_type, rs, perplexity: (latent_embed_plot_umap(xs=z if data_type == 'z' and function == 'latent_embed_plot_umap' else (mu if data_type == 'mu' else p),\n",
    "                                                                ys=labels, classes=class_list_order,  rs=rs, marker_size=100, l_w= 2.5,  display=True)\n",
    "                                          if function == 'latent_embed_plot_umap'\n",
    "                                          else latent_embed_plot_tsne(xs=z if data_type == 'z' and function == 'latent_embed_plot_tsne' else mu,\n",
    "                                                                      ys=labels, perplexity=perplexity, marker_size=100, l_w= 2.5, display=True)),\n",
    "        function=function_selector,\n",
    "        data_type=Dropdown(options=['z', 'mu', 'p'], value='z', description='Data Type'),\n",
    "        rs=random_state_slider,\n",
    "        perplexity = perplexity_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd46bd9",
   "metadata": {},
   "source": [
    "## Creating new latent interpolation plots\n",
    "\n",
    "1. choose the size of your input images : `dsize`\n",
    "\n",
    "2. choose the plot interpolation steps via the slider : `num_int`\n",
    "\n",
    "note that everytime you drag the `num_int` slider the plot corners change, if you would like to start from a given number of interpolation steps change the value of `init_interpolation_steps` in the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c387be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad020e6ac0a34f19b09f3db916de3ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='num_int', max=20, min=2), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(num_int)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from avae.vis import latent_4enc_interpolate_plot\n",
    "\n",
    "dsize = [32,32]\n",
    "init_interpolation_steps = 10\n",
    "\n",
    "number_interpolation = IntSlider(min=2, max=20, step=1, value=init_interpolation_steps)\n",
    "interact(lambda num_int: latent_4enc_interpolate_plot(dsize= dsize, xs= torch.from_numpy(z).to(dtype=torch.float), \n",
    "                             ys= labels, vae = model ,device = \"cpu\", plots_config = f\"1,{num_int}\", \n",
    "                             poses = p, display = True),num_int=number_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1fbf5",
   "metadata": {},
   "source": [
    "## Creating new pose interpolation plots\n",
    "1. choose the size of your input images : `dsize`\n",
    "2. choose the plot interpolation steps via the slider : `number_of_samples`\n",
    "3. choose the classes you would like the interpolation to be generated for via the variable : `pose_vis_class`\n",
    "4. `specific_enc` : If you do not wish to use a specific encoding, set this variable to `None`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c13d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/path/to/image\"\n",
    "img = NormalizeData(np.load(img_path))\n",
    "plt.imshow(img)\n",
    "_, specific_enc, _, _, _ = model(torch.from_numpy(img[np.newaxis,np.newaxis, ...]).to(torch.float32))     \n",
    "\n",
    "specific_enc = specific_enc.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avae.vis import pose_class_disentanglement_plot\n",
    "\n",
    "dsize= [32,32]\n",
    "pose_vis_class = \"1,2,3,4,5,6,7,8,9\"\n",
    "specific_enc = None\n",
    "\n",
    "num_int = IntSlider(min=2, max=20, step=1, value=8)\n",
    "interact(lambda number_of_samples:pose_class_disentanglement_plot(dsize= dsize, x= torch.from_numpy(z).to(dtype=torch.float), \n",
    "                                y= labels, pose_vis_class=pose_vis_class, poses = p, vae = model,\n",
    "                                device = \"cpu\", number_of_samples = number_of_samples, specific_enc= specific_enc, display = True), number_of_samples = num_int )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e93b5",
   "metadata": {},
   "source": [
    "# Plot Affinity Matrix \n",
    "\n",
    "1. `affinity_path`: The path to affinity matrix \n",
    "2. `classes`: The classes you would like to represent in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_path = \"/path/to/affinity/affinity.csv\"\n",
    "classes = \"aebdiju2\"\n",
    "classes=list(classes)\n",
    "\n",
    "lookup = pd.read_csv(affinity_path, header=0)\n",
    "\n",
    "lookup = lookup.loc[[lookup.columns.get_loc(c) for c in classes if c in lookup],classes]\n",
    "\n",
    "\n",
    "with plt.rc_context(\n",
    "    {\"font.weight\": \"bold\", \"font.size\": int(len(classes) / 3) + 3}\n",
    "):\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(int(len(classes)) / 2, int(len(classes)) / 2)\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Affinity Matrix\")\n",
    "im = ax.imshow(lookup, vmin=-1, vmax=1, cmap=plt.cm.get_cmap(\"RdBu\"))\n",
    "ax.set_xticks(np.arange(0, len(classes)))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticks(np.arange(0, len(classes)))\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "ax.tick_params(axis=\"x\", rotation=90, labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "pos = ax.get_position()\n",
    "\n",
    "# Set the height of the color bar to match the height of the plot\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_ticks([-1, 0, 1])  # You can customize the ticks as needed\n",
    "\n",
    "# Adjust the position of the color bar to match the height of the plot\n",
    "cbar.ax.set_position([pos.x1 - 0.01, pos.y0, 0.02, pos.height-1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4e4f8",
   "metadata": {},
   "source": [
    "# Plot the cosine similarity of the latent space\n",
    "Choose from the dropdown menu between plotting the average or the standard deviation of the cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ee446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avae.vis import latent_space_similarity_plot\n",
    "\n",
    "interact(lambda plot_mode: latent_space_similarity_plot(latent_space= z, class_labels = labels, \n",
    "                             classes_order= classes,plot_mode = plot_mode, \n",
    "                             display = True, font_size=16, dpi=100), plot_mode=Dropdown(options=[\"mean\", \"std\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea49af",
   "metadata": {},
   "source": [
    "# Plot Rotation Angle-Pose correlation    \n",
    "\n",
    "This figure takes in an original image and rotates it through a range of angle from `THETA_LOWER` to `THETA_UPPER` and extract the pose value learnt. It then plots the corresponding pose value and the angle of rotation as well as the decoded image grid.\n",
    "\n",
    "- `img_path`: Input the path of an image from your data \n",
    "- `THETA_LOWER`, `THETA_UPPER`: Input the lower and upper range of rotation in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "from skimage.util import montage \n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "img_path = \"/path/to/image\"\n",
    "THETA_LOWER = -30\n",
    "THETA_UPPER = 30\n",
    "img = NormalizeData(np.load(img_path))\n",
    "\n",
    "r=[]\n",
    "original = []\n",
    "pose_angle=[]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for theta in range(THETA_LOWER,THETA_UPPER):\n",
    "        image = rotate(img, theta, order=0, reshape = False)\n",
    "        x, z, z_pose, mu, logvar = model(torch.from_numpy(image[np.newaxis,np.newaxis, ...]).to(torch.float32))     \n",
    "        \n",
    "        if theta % 5 == 0 : \n",
    "            r.append(x.squeeze().detach().numpy())\n",
    "            original.append(image.squeeze())\n",
    "        pose_angle.append(z_pose[0,0].cpu().clone().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(10,4))\n",
    "ax1, ax2, ax3 = ax\n",
    "\n",
    "coef = np.polyfit(range(THETA_LOWER,THETA_UPPER),np.squeeze(pose_angle),1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "ax1.plot(range(THETA_LOWER,THETA_UPPER),np.squeeze(pose_angle),'ro',range(THETA_LOWER,THETA_UPPER),poly1d_fn(range(THETA_LOWER,THETA_UPPER)),'--k',\n",
    "    linewidth=2)\n",
    "ax1.set_xlabel(r'$\\theta$')\n",
    "ax1.set_ylabel('pose')\n",
    "\n",
    "r = np.stack(r, axis=0)\n",
    "original = np.stack(original, axis=0)\n",
    "m = montage(r, grid_shape=(3, 6))\n",
    "n = montage(original, grid_shape=(3, 6))\n",
    "ax2.imshow(m); ax3.imshow(n)\n",
    "ax2.axis('off'); ax3.axis('off')\n",
    "ax2.set_title(\" images\"); ax3.set_title(\"Original images\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6247a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c9541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6247a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tomlavae_env",
   "language": "python",
   "name": "new_tomlavae_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
