{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314eccba",
   "metadata": {},
   "source": [
    "# Post processing notebook \n",
    "\n",
    "### This notebook import the visualisation functions from the main library and loads the state and meta file and allows you to modify the appearance of the output of Affinity-VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, fixed, Dropdown\n",
    "from avae.vis import latent_embed_plot_umap, latent_embed_plot_tsne\n",
    "from avae.utils import colour_per_class\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf4e2d",
   "metadata": {},
   "source": [
    "### The following functions take in the path of the meta and the state file respectively and load them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def load_meta(meta_fn, device=\"cpu\"):\n",
    "    meta_df = pd.read_pickle(meta_fn)\n",
    "\n",
    "    mu = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"lat\")]\n",
    "    ].to_numpy()  # Assuming the column name for latent variables is 'latent'\n",
    "    \n",
    "    labels = meta_df[\"id\"]\n",
    "\n",
    "    pose = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"pos\")]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    std = meta_df[\n",
    "        [col for col in meta_df.columns if col.startswith(\"std\")]\n",
    "    ].to_numpy()\n",
    "    \n",
    "    #### why is there an 0.5 attached to beginning of each std array so that the size is 17?\n",
    "    std = std[:,1:]\n",
    "    \n",
    "    z =  np.random.randn(*std.shape)  * std + mu\n",
    "    \n",
    "    return mu, pose, labels, std, z\n",
    "\n",
    "\n",
    "def load_model(model_fn, device=\"cpu\"):\n",
    "    checkpoint = torch.load(model_fn, map_location=torch.device(device))\n",
    "    model = checkpoint[\"model_class_object\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed324a89",
   "metadata": {},
   "source": [
    "# Enter the path for the saved model and corresponding meta file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = 'path/to/model'\n",
    "meta_fn = 'path/to/meta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcb862",
   "metadata": {},
   "source": [
    "# Grab the Information in the meta file and load the model\n",
    "### From the meta file \n",
    "1. `mu`: mean of the latents ($\\mu$)\n",
    "2. `std`: standard deviation of the latents ($\\sigma$)\n",
    "3. `p`: $pose$\n",
    "4. `labels`: class labels\n",
    "5. `z`: sampled latent ($z$) \n",
    "### model loaded to `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26168d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, p, labels, std, z = load_meta(meta_fn)\n",
    "model =load_model(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cebd8",
   "metadata": {},
   "source": [
    "# Configure latent embedding:\n",
    "#### select the following to plot the UMAP embedding of the latent space : \n",
    "1. `select_function`: The drop down list allows you to choose between UMAP and TSNE for your dimentionality reduction of the latent embeddings. \n",
    "2. `rs` : This slider sets the random state for the UMAP plot.\n",
    "3. `perplexity`: This slider sets the perplexity for the TSNE plot.\n",
    "4. `Data Type`: This drop down list allows you to select which variable to plot (`z` : stochastic sample of latent space, `mu`: the mean of the latent space, `p`: Pose space)\n",
    "5. `class_list_order`: order of the classes, this should help with consistent colouring of the plots. This is a list.\n",
    "6. `marker_size`: adjust the size of markers for each data point to have the desired plot appearence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slider widget\n",
    "class_list_order = list(\"0123456789\")\n",
    "random_state_slider = IntSlider(min=0, max=100, step=1, value=42)\n",
    "perplexity_slider = IntSlider(min=2, max=100, step=1, value=40, description='Perplexity')\n",
    "\n",
    "# Define the dropdown widget for selecting the function\n",
    "function_selector = Dropdown(options=['latent_embed_plot_umap', 'latent_embed_plot_tsne'],\n",
    "                             value='latent_embed_plot_umap', description='Select Function')\n",
    "\n",
    "# Use the interact function with both widgets\n",
    "interact(lambda function, data_type, rs, perplexity: (latent_embed_plot_umap(xs=z if data_type == 'z' and function == 'latent_embed_plot_umap' else (mu if data_type == 'mu' else p),\n",
    "                                                                  ys=labels, classes=class_list_order,  rs=rs, marker_size=24, l_w= 2.5,  display=True)\n",
    "                                          if function == 'latent_embed_plot_umap'\n",
    "                                          else latent_embed_plot_tsne(xs=z if data_type == 'z' and function == 'latent_embed_plot_tsne' else mu,\n",
    "                                                                      ys=labels, perplexity=perplexity, display=True)),\n",
    "         function=function_selector,\n",
    "         data_type=Dropdown(options=['z', 'mu', 'p'], value='z', description='Data Type'),\n",
    "         rs=random_state_slider,\n",
    "         perplexity = perplexity_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd46bd9",
   "metadata": {},
   "source": [
    "## Creating new latent interpolation plots\n",
    "\n",
    "1. choose the size of your input images : `dsize`\n",
    "\n",
    "2. choose the plot interpolation steps via the slider : `num_int`\n",
    "\n",
    "note that everytime you drag the `num_int` slider the plot corners change, if you would like to start from a given number of interpolation steps change the value of `init_interpolation_steps` in the code cell below.\n",
    "\n",
    "\n",
    "Note that for 3D data, you can set `display=False` and have a `.mrc` file saved to a plot directoty. This still will allow you to modify the appearence of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avae.vis import latent_4enc_interpolate_plot\n",
    "\n",
    "dsize = [32,32]\n",
    "init_interpolation_steps = 8\n",
    "\n",
    "number_interpolation = IntSlider(min=2, max=20, step=1, value=init_interpolation_steps)\n",
    "interact(lambda num_int: latent_4enc_interpolate_plot(dsize= dsize, xs= torch.from_numpy(z).to(dtype=torch.float), \n",
    "                             ys= labels, vae = model ,device = \"cpu\", plots_config = f\"1,{num_int}\", \n",
    "                             poses = p, display = True),num_int=number_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1fbf5",
   "metadata": {},
   "source": [
    "## Creating new pose interpolation plots\n",
    "\n",
    "you have two options here:\n",
    "- Load a specific image (`img_path`), encode this specific image and use the specific encoding from this image in your pose interpolation.\n",
    "- Not load a specific image, pass the whole `z` and the interpolation function would choose one of the encodings from your selected classes at random. \n",
    "\n",
    "1. choose the size of your input images : `dsize`\n",
    "2. choose the plot interpolation steps via the slider : `number_of_samples`\n",
    "3. choose the classes you would like the interpolation to be generated for via the variable : `pose_vis_class`\n",
    "4. `specific_enc` : If you do not wish to use a specific encoding, set this variable to `None`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/path/to/image\"\n",
    "img = NormalizeData(np.load(img_path))\n",
    "plt.imshow(img)\n",
    "_, specific_enc, _, _, _ = model(torch.from_numpy(img[np.newaxis,np.newaxis, ...]).to(torch.float32))     \n",
    "\n",
    "specific_enc = specific_enc.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avae.vis import pose_class_disentanglement_plot\n",
    "\n",
    "dsize= [32,32]\n",
    "pose_vis_class = \"3\"\n",
    "#specific_enc = None\n",
    "\n",
    "num_int = IntSlider(min=2, max=20, step=1, value=8)\n",
    "interact(lambda number_of_samples:pose_class_disentanglement_plot(dsize= dsize, x= torch.from_numpy(z).to(dtype=torch.float), \n",
    "                                y= labels, pose_vis_class=pose_vis_class, poses = p, vae = model,\n",
    "                                device = \"cpu\", number_of_samples = number_of_samples, specific_enc= specific_enc, display = True), number_of_samples = num_int )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e93b5",
   "metadata": {},
   "source": [
    "# Plot Affinity Matrix \n",
    "\n",
    "1. `affinity_path`: The path to affinity matrix \n",
    "2. `classes`: The classes you would like to represent in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_path = \"/path/to/affinity/affinity.csv\"\n",
    "classes = \"aebd2ziju\"\n",
    "classes=list(classes)\n",
    "\n",
    "lookup = pd.read_csv(affinity_path, header=0)\n",
    "\n",
    "lookup = lookup.loc[[lookup.columns.get_loc(c) for c in classes if c in lookup],classes]\n",
    "\n",
    "\n",
    "with plt.rc_context(\n",
    "    {\"font.weight\": \"bold\", \"font.size\": int(len(classes) / 3) + 3}\n",
    "):\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(int(len(classes)) / 2, int(len(classes)) / 2)\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Affinity Matrix\")\n",
    "im = ax.imshow(lookup, vmin=-1, vmax=1, cmap=plt.cm.get_cmap(\"RdBu\"))\n",
    "ax.set_xticks(np.arange(0, len(classes)))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticks(np.arange(0, len(classes)))\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "ax.tick_params(axis=\"x\", rotation=90, labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "pos = ax.get_position()\n",
    "\n",
    "# Set the height of the color bar to match the height of the plot\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_ticks([-1, 0, 1])  # You can customize the ticks as needed\n",
    "\n",
    "# Adjust the position of the color bar to match the height of the plot\n",
    "cbar.ax.set_position([pos.x1 - 0.01, pos.y0, 0.02, pos.height-1])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"affinity.png\",dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4e4f8",
   "metadata": {},
   "source": [
    "# Plot the cosine similarity of the latent space\n",
    "Choose from the dropdown menu between plotting the average or the standard deviation of the cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ee446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avae.vis import latent_space_similarity_plot\n",
    "\n",
    "interact(lambda plot_mode: latent_space_similarity_plot(latent_space= z, class_labels = labels, \n",
    "                             classes_order= classes,plot_mode = plot_mode, \n",
    "                             display = True, font_size=16, dpi=100), plot_mode=Dropdown(options=[\"mean\", \"std\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea49af",
   "metadata": {},
   "source": [
    "# Plot Rotation Angle-Pose correlation    \n",
    "\n",
    "This figure takes in an original image and rotates it through a range of angle from `THETA_LOWER` to `THETA_UPPER` and extract the pose value learnt. It then plots the corresponding pose value and the angle of rotation as well as the decoded image grid.\n",
    "\n",
    "- `img_path`: Input the path of an image from your data \n",
    "- `THETA_LOWER`, `THETA_UPPER`: Input the lower and upper range of rotation in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "from skimage.util import montage \n",
    "\n",
    "img_path = \"/path/to/image\"\n",
    "THETA_LOWER = -40\n",
    "THETA_UPPER = 40\n",
    "theta_list = np.arange(THETA_LOWER,THETA_UPPER,0.2)\n",
    "img = NormalizeData(np.load(img_path))\n",
    "\n",
    "r=[]\n",
    "original = []\n",
    "pose_angle=[]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for idx, theta in enumerate(theta_list):\n",
    "        image = rotate(img, theta, order=0, reshape = False)\n",
    "        \n",
    "        x, z, z_pose, mu, logvar = model(torch.from_numpy(image[np.newaxis,np.newaxis, ...]).to(torch.float32))     \n",
    "        if idx % 5 == 0 : \n",
    "            r.append(x.squeeze().detach().numpy())\n",
    "            original.append(image.squeeze())\n",
    "        pose_angle.append(z_pose[0,0].cpu().clone().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1, ax2 = ax\n",
    "\n",
    "coef = np.polyfit(theta_list,np.squeeze(pose_angle),1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "ax1.plot(theta_list,np.squeeze(pose_angle),'ro',theta_list,poly1d_fn(theta_list),'--k',\n",
    "    linewidth=2)\n",
    "ax1.set_xlabel(r'$\\theta$', fontsize=18)\n",
    "ax1.set_ylabel('pose', fontsize=18)\n",
    "ax1.tick_params(axis='both',labelsize=18)\n",
    "\n",
    "r = np.stack(r, axis=0)\n",
    "original = np.stack(original, axis=0,)\n",
    "\n",
    "num_rows = 10\n",
    "m = montage(r, grid_shape=( int(len(r)/num_rows), num_rows))\n",
    "n = montage(original, grid_shape=( int(len(r)/num_rows), num_rows))\n",
    "ax2.imshow(m)\n",
    "#ax3.imshow(n)\n",
    "ax2.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"pose_angle_images.png\", dpi=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0d6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8406d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tomlavae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
